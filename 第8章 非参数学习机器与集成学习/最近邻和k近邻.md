### 判别函数与最近邻法的渐近错误率

在分类问题中，**判别函数**是用于决策的关键工具。我们通常希望通过对每个类定义判别函数来进行分类。对于**最近邻法**，判别函数可以基于样本与各类中最近样本的距离来定义。

#### 判别函数的形式

对于类 $\omega_i$，其判别函数 $g_i(x)$ 可以写作：

$
g_i(x) = \min_{x_j \in \omega_i} \delta(x, x_j), \quad i = 1, \dots, c
$

其中，$\delta(x, x_j)$ 表示样本 $x$ 与类 $\omega_i$ 中样本 $x_j$ 之间的距离。即，对于每个新样本 $x$，我们通过计算它与类 $\omega_i$ 中所有样本的距离，找到最近的一个样本并将该距离作为判别函数的输出。

#### 分类决策规则

根据各类的判别函数，我们的分类决策规则为：如果样本 $x$ 对应的判别函数 $g_k(x)$ 最小，即：

$
\text{若 } g_k(x) = \min_{i=1, \dots, c} g_i(x), \text{则 } x \in \omega_k
$

这意味着，新的样本 $x$ 将被分配到距离它最近的已知类别 $\omega_k$。

#### 距离度量与相似性度量

在实际应用中，我们可以根据具体问题选择不同的距离度量，例如欧氏距离、曼哈顿距离等。此外，距离度量还可以转化为相似性度量。在这种情况下，决策规则将从“最小距离”转换为“最大相似度”，即选择与新样本最相似的已知样本的类别作为预测类别。

#### 最近邻法的渐近错误率

研究表明，**最近邻法**在已知样本数足够多时能够取得良好的效果。对于最近邻法的错误率，有如下理论结果。

设 $N$ 个样本下，最近邻法的平均错误率为 $P_N(e)$，样本 $x$ 的最近邻为 $x'$，则平均错误率可以写成：

$
P_N(e) = \iint P_N(e \mid x, x') p(x' \mid x) \, dx' p(x) \, dx
$

#### 渐近错误率与贝叶斯错误率

当样本数 $N \to \infty$ 时，定义最近邻法的渐近错误率 $P$ 为 $P_N(e)$ 的极限，即：

$
P = \lim_{N \to \infty} P_N(e)
$

这个渐近错误率与 **贝叶斯错误率** $P^*$ 之间有如下关系：

$
P^* \leqslant P \leqslant P^*\left(2 - \frac{c}{c - 1} P^*\right)
$

其中，$P^*$ 为贝叶斯错误率，$c$ 为类别数。

贝叶斯错误率是理论上最优的错误率，代表了最理想的分类器可以达到的性能。而最近邻法的渐近错误率总会落在贝叶斯错误率和其两倍之间。图 8-1 表示了这种关系，阴影区域为渐近错误率的可能范围。

这一结论说明，在样本数趋于无限时，最近邻法的错误率最坏情况下不会超过贝叶斯错误率的两倍，最好的情况下可以接近或达到贝叶斯错误率。然而，需要注意的是，这一结论是在样本数足够大时成立的。

---

### 选择题及答案

1. **最近邻法的判别函数如何定义？**
   - A. 样本到最近已知样本的距离  
   - B. 样本到所有已知样本的平均距离  
   - C. 样本到最远已知样本的距离  
   - D. 样本与所有已知样本的差异  
   - **正确答案**: A  
   - **解释**: 最近邻法的判别函数通过样本到其最近的已知样本的距离进行分类。

2. **在判别函数形式下，判别决策的准则是什么？**
   - A. 判别函数的最大值  
   - B. 判别函数的最小值  
   - C. 判别函数的平均值  
   - D. 判别函数的随机值  
   - **正确答案**: B  
   - **解释**: 决策准则是通过比较各类的判别函数，选择最小值对应的类别。

3. **贝叶斯错误率代表什么？**
   - A. 分类的最优错误率  
   - B. 分类的最差错误率  
   - C. 随机分类的错误率  
   - D. 最近邻法的实际错误率  
   - **正确答案**: A  
   - **解释**: 贝叶斯错误率代表理论上最优的分类器能够达到的错误率。

4. **最近邻法的渐近错误率与贝叶斯错误率的关系是什么？**
   - A. 最近邻法的错误率总是等于贝叶斯错误率  
   - B. 最近邻法的错误率介于贝叶斯错误率和其两倍之间  
   - C. 最近邻法的错误率总是大于贝叶斯错误率两倍  
   - D. 最近邻法的错误率与贝叶斯错误率无关  
   - **正确答案**: B  
   - **解释**: 最近邻法的渐近错误率最差不超过贝叶斯错误率的两倍，最好接近贝叶斯错误率。

5. **在渐近情况下，最近邻法的错误率会如何变化？**
   - A. 趋向于随机分类错误率  
   - B. 逐渐接近贝叶斯错误率  
   - C. 与样本数量无关  
   - D. 随机变化  
   - **正确答案**: B  
   - **解释**: 随着样本数趋于无限，最近邻法的错误率会逐渐接近贝叶斯错误率。

6. **当我们使用相似性度量替代距离度量时，决策规则如何变化？**
   - A. 选择与样本最相似的类别  
   - B. 选择距离样本最远的类别  
   - C. 选择随机类别  
   - D. 不再使用判别函数  
   - **正确答案**: A  
   - **解释**: 当使用相似性度量时，决策规则会转变为选择与样本最相似的类别。

7. **最近邻法中的距离度量通常用来计算什么？**
   - A. 样本间的相似性  
   - B. 样本间的差异  
   - C. 样本到类别中心的距离  
   - D. 样本到最近样本的距离  
   - **正确答案**: D  
   - **解释**: 距离度量用来计算样本到已知样本集中最近样本的距离。

8. **贝叶斯错误率是如何影响分类性能的？**
   - A. 它决定了分类的最优上限  
   - B. 它是分类的最差情况  
   - C. 它没有影响  
   - D. 它仅影响无监督学习  
   - **正确答案**: A  
   - **解释**: 贝叶斯错误率是分类器性能的理论上限，表示最优分类器可以达到的最低错误率。

9. **当类别数 $c$ 增加时，最近邻法的错误率如何变化？**
   - A. 减少  
   - B. 增加  
   - C. 不变  
   - D. 随机变化  
   - **正确答案**: B  
   - **解释**: 当类别数增加时，分类任务变得更难，错误率通常会增加。

10. **在最近邻法中，决策依据的主要标准是什么？**
    - A. 样本的特征数量  
    - B. 样本的标签分布  
    - C. 样本之间的距离或相似性  
    - D. 样本的均值  
    - **正确答案**: C  
    - **解释**: 最近邻法根据样本之间的距离或相似性进行分类决策。

11. **最近邻法在样本数量趋于无穷大时表现如何？**
    - A. 变得无效  
    - B. 渐近接近贝叶斯错误率  
    - C. 随机分类  
    - D. 错误率增加  
    - **正确答案**: B  
    - **解释

**: 当样本数量趋于无穷大时，最近邻法的错误率渐近接近贝叶斯错误率。

12. **贝叶斯错误率对应的是哪种分类器的错误率？**
    - A. 最近邻分类器  
    - B. 最优分类器  
    - C. 随机分类器  
    - D. 线性分类器  
    - **正确答案**: B  
    - **解释**: 贝叶斯错误率对应的是最优分类器能够达到的最低错误率。

13. **在最近邻法中，如何处理高维数据的分类问题？**
    - A. 使用特征选择降低维度  
    - B. 使用更多类别  
    - C. 增加样本数量  
    - D. 随机选择特征  
    - **正确答案**: A  
    - **解释**: 高维数据可能导致距离计算失效，特征选择可以降低维度，改善分类效果。

14. **最近邻法的渐近错误率与贝叶斯错误率的上下界由什么决定？**
    - A. 样本数量  
    - B. 类别数 $c$  
    - C. 特征数量  
    - D. 分类器类型  
    - **正确答案**: B  
    - **解释**: 最近邻法的渐近错误率上下界由类别数 $c$ 决定。

15. **当类别数 $c$ 较大时，最近邻法的最坏错误率是多少？**
    - A. 贝叶斯错误率的两倍  
    - B. 贝叶斯错误率的三倍  
    - C. 随机错误率  
    - D. 0%  
    - **正确答案**: A  
    - **解释**: 当类别数较大时，最近邻法的最坏错误率为贝叶斯错误率的两倍。


### $k$-近邻法（$k$-Nearest Neighbors, k-NN）

在**最近邻法**中，分类决策是基于新样本与已知样本集中最近的一个样本的距离。然而，当数据分布复杂或噪声严重时，这种方法可能存在一定的风险，可能会因为噪声样本导致错误分类。为了解决这一问题，**$k$-近邻法**引入了一种投票机制，通过选择与新样本最近的多个已知样本，并根据这些样本的类别投票来决定新样本的类别。

#### $k$-近邻法的定义
$k$-近邻法的核心思想是：对于一个新样本，选择前 $k$ 个与其距离最近的已知样本，然后根据这些近邻样本所属的类别进行投票，决定新样本的类别。如果参与投票的 $k$ 个样本中，类别 $\omega_i$ 的样本数量为 $k_i$，则定义类别 $\omega_i$ 的判别函数为：

$
g_i(x) = k_i, \quad i = 1, \cdots, c
$

其中，$k_i$ 是属于类别 $\omega_i$ 的近邻样本数，$c$ 是类别的总数。

#### 决策规则
根据判别函数的值，$k$-近邻法的决策规则是：选择判别函数值 $g_i(x)$ 最大的类别作为新样本的分类结果。如果有多个类别的 $g_i(x)$ 相等，通常需要引入其他决策规则，例如随机选择一个类别或根据某些优先级选择。

$
\text{若 } g_k(x) = \max_{i=1, \dots, c} g_i(x), \text{则 } x \in \omega_k
$

#### 渐近错误率分析
对于 $k$-近邻法，其渐近错误率仍然满足与**贝叶斯错误率**的上下界关系，即：

$
P^* \leqslant P \leqslant P^*\left(2 - \frac{c}{c - 1}P^*\right)
$

随着 $k$ 的增加，渐近错误率的上界逐渐降低。当 $k$ 逐渐趋于无穷大时，$k$-近邻法的渐近错误率将会接近贝叶斯错误率。因此，理论上来说，随着 $k$ 的增加，分类结果会越来越准确，但前提是样本数量必须足够多。

然而，尽管 $k$ 可以增加，但 $k$ 仍然必须是样本总数中可以忽略的一小部分，这类似于在非参数方法中估计概率密度时的要求。

#### 实际应用中的 $k$ 值选择
在实际应用中，$k$ 的值需要根据数据的具体情况进行选择。一般来说，选择样本总数的一个很小的比例作为 $k$ 即可。对于二分类问题，通常选择 $k$ 为奇数，以避免分类结果中出现两类得票数相等的情况。

---

### 选择题及答案

1. **$k$-近邻法的核心思想是什么？**
   - A. 使用单个最近邻样本进行分类  
   - B. 通过多个最近邻样本投票进行分类  
   - C. 使用所有样本进行分类  
   - D. 通过决策树构建分类器  
   - **正确答案**: B  
   - **解释**: $k$-近邻法通过选择前 $k$ 个最近的样本投票决定新样本的类别。

2. **在 $k$-近邻法中，$k$ 代表什么？**
   - A. 最近的样本数量  
   - B. 类别的数量  
   - C. 数据的维度  
   - D. 样本的特征数  
   - **正确答案**: A  
   - **解释**: $k$ 代表用于投票的最近邻样本的数量。

3. **最近邻法可以被看作是 $k$-近邻法的哪种特例？**
   - A. $k=3$  
   - B. $k=5$  
   - C. $k=10$  
   - D. $k=1$  
   - **正确答案**: D  
   - **解释**: 最近邻法是 $k=1$ 时的特例，即仅考虑距离最近的一个样本进行分类。

4. **在 $k$-近邻法中，判别函数的形式是什么？**
   - A. 类别权重的和  
   - B. 样本到类别中心的距离  
   - C. 属于每个类别的近邻样本数量  
   - D. 每个类别的平均距离  
   - **正确答案**: C  
   - **解释**: 判别函数表示属于每个类别的最近邻样本的数量 $k_i$。

5. **$k$-近邻法的决策规则是什么？**
   - A. 选择最远的样本类别  
   - B. 选择最大投票数的类别  
   - C. 随机选择一个类别  
   - D. 选择所有类别的均值  
   - **正确答案**: B  
   - **解释**: $k$-近邻法通过选择投票数最多的类别作为新样本的分类结果。

6. **对于 $k$-近邻法，如何处理分类结果中的投票平局情况？**
   - A. 选择随机类别  
   - B. 增加 $k$ 的值  
   - C. 选择某个优先级高的类别  
   - D. 放弃分类  
   - **正确答案**: C  
   - **解释**: 当投票平局时，通常会根据某些优先级规则来选择类别。

7. **$k$-近邻法的渐近错误率随着 $k$ 增加会如何变化？**
   - A. 增加  
   - B. 减少  
   - C. 不变  
   - D. 随机变化  
   - **正确答案**: B  
   - **解释**: 随着 $k$ 增加，渐近错误率逐渐接近贝叶斯错误率，即错误率减少。

8. **当 $k$ 趋于无穷大时，$k$-近邻法的渐近错误率会接近什么？**
   - A. 随机分类错误率  
   - B. 贝叶斯错误率  
   - C. 0% 错误率  
   - D. 50% 错误率  
   - **正确答案**: B  
   - **解释**: 当 $k$ 趋于无穷大时，$k$-近邻法的错误率会接近贝叶斯错误率，这是最优分类器的理论错误率。

9. **在 $k$-近邻法中，通常选择 $k$ 为奇数的原因是什么？**
   - A. 为了避免分类结果中投票数相等的情况  
   - B. 为了提高计算效率  
   - C. 为了增加投票样本数量  
   - D. 为了减少错误率  
   - **正确答案**: A  
   - **解释**: 选择 $k$ 为奇数是为了避免在二分类问题中投票数相等的情况。

10. **对于 $k$-近邻法的渐近错误率，哪种错误率为其理论最优值？**
    - A. 贝叶斯错误率  
    - B. 随机分类错误率  
    - C. 过拟合错误率  
    - D. 欠拟合错误率  
    - **正确答案**: A  
    - **解释**: 贝叶斯错误率是理论上最优分类器可以达到的最小错误率。

11. **当 $k$ 的值太小时，$k$-近邻法可能会遇到什么问题？**
    - A. 过拟合  
    - B. 欠拟合  
    - C. 分类效率降低  
    - D. 错误率上升  
    - **正确答案**: A  
    - **解释**: 当 $k$ 太小时，模型容易过拟合，因为它仅考虑极少量的近邻样本，可能受到噪声的影响。

12. **当 $k$ 的值太大时，$k$-近邻法可能会遇到什么问题？**
    - A. 过拟合  
    - B. 欠拟合  
    - C. 无法分类  
    - D. 错误率上升  
    - **正确答案**: B  
    - **解释**: 当 $k$ 过大时，模型容易欠拟合，因为它

考虑了过多的近邻样本，可能导致分类边界过于平滑。

13. **$k$-近邻法在计算过程中需要依赖于什么？**
    - A. 类别分布  
    - B. 样本间的距离计算  
    - C. 样本的标签分布  
    - D. 训练样本数量  
    - **正确答案**: B  
    - **解释**: $k$-近邻法通过计算样本间的距离来决定哪些样本参与投票。

14. **$k$-近邻法通常使用的距离度量是什么？**
    - A. 曼哈顿距离  
    - B. 欧氏距离  
    - C. 余弦相似度  
    - D. 相关系数  
    - **正确答案**: B  
    - **解释**: 欧氏距离是 $k$-近邻法中常用的距离度量，用于计算样本之间的几何距离。

15. **$k$-近邻法是一种什么类型的算法？**
    - A. 参数化算法  
    - B. 非参数化算法  
    - C. 深度学习算法  
    - D. 生成式算法  
    - **正确答案**: B  
    - **解释**: $k$-近邻法是一种非参数化算法，它不需要预先训练模型或学习参数。

### 近邻法的快速算法

**近邻法**（包括最近邻法和 $k$-近邻法）是一种简单但计算量大的分类方法。其基本思想是通过比较新样本与已知样本之间的距离，找到最近的一个或几个样本，然后根据这些样本的类别来决定新样本的类别。虽然直观易懂，但由于需要将新样本与所有已知样本进行比较，计算和存储成本很高，尤其是在数据量较大的情况下，计算效率成为了瓶颈。

为了提高近邻法的计算性能，研究者们设计了一些 **快速算法**。这些算法的核心思想是通过某种结构将样本数据分层组织，使得在寻找最近邻时，能够跳过不可能的区域，从而减少计算量。以下介绍其中的一种方法：**分枝定界算法**（Branch and Bound Algorithm）。

#### 1. 分枝定界算法的基本思想
**分枝定界算法**的主要思想是将已知的样本集分为多个子集，形成一个树状结构。通过这种分层的组织方式，新样本不再需要与每个已知样本进行比较，而是可以逐层排除不可能包含最近邻的子集，只在最后的节点中进行精确比较。

该方法分为两个主要阶段：

#### 第一阶段：构建样本的树状结构
在第一阶段，将样本集分层次划分，形成一个树状结构。每个节点代表一个子集，节点包含以下信息：
- **$ \vartheta_{p}^{.} $**：节点 $ \rho $ 对应的样本子集。
- **$ N_p $**：该节点对应的样本数量。
- **$ M_p $**：该节点样本子集的均值（即该节点的“中心”）。
- **$ r_p $**：从该节点的均值 $ M_p $ 到子集中最远样本的距离。

通过这种树状结构，可以对大规模样本集进行分层划分。图 8-3 展示了样本集的层级划分，形成了一棵树。每个节点代表一个样本子集，中心和最远距离作为节点的特征。

#### 第二阶段：搜索最近邻样本
在第二阶段，基于构建的树状结构，使用搜索算法寻找新样本的最近邻样本。该搜索过程利用树结构的层次特性，逐步缩小搜索范围，跳过不可能包含最近邻的节点，从而显著减少计算量。

#### 核心规则
在搜索阶段，引入了两个核心规则，用于判断未知样本的最近邻是否在某个节点中。这两个规则通过样本之间的距离和节点的属性（如中心和最远距离）来排除无关的节点。

通过这两个阶段的处理，近邻法的计算效率得到了显著提升。

---

### 选择题及答案

1. **近邻法的快速算法的核心思想是什么？**
   - A. 使用全部样本进行比较  
   - B. 通过分层划分样本集减少比较次数  
   - C. 通过增加计算复杂度提高精度  
   - D. 仅使用随机样本进行比较  
   - **正确答案**: B  
   - **解释**: 近邻法的快速算法通过分层划分样本集，使得在搜索最近邻时可以跳过不相关的部分，从而减少比较次数。

2. **快速近邻法的分枝定界算法属于哪种方法？**
   - A. 随机算法  
   - B. 回溯算法  
   - C. 递归分割算法  
   - D. 贪心算法  
   - **正确答案**: C  
   - **解释**: 分枝定界算法通过递归地分割样本集形成树状结构，从而加速搜索过程。

3. **在分枝定界算法中，节点的属性包括以下哪一项？**
   - A. 样本数量  
   - B. 每个样本的权重  
   - C. 节点样本的中心和最远距离  
   - D. 节点样本的特征向量  
   - **正确答案**: C  
   - **解释**: 每个节点包含样本子集的均值（中心）和最远距离，用于搜索时判断是否需要进一步搜索该节点。

4. **在快速近邻法中，样本集被分成多个子集形成什么结构？**
   - A. 线性结构  
   - B. 环形结构  
   - C. 树状结构  
   - D. 矩阵结构  
   - **正确答案**: C  
   - **解释**: 样本集被分层划分为一个树状结构，以便在搜索时逐层排除不相关的节点。

5. **分枝定界算法中的“分枝”指的是？**
   - A. 每个样本的特征分割  
   - B. 样本集的层次划分  
   - C. 分类的概率计算  
   - D. 决策函数的优化  
   - **正确答案**: B  
   - **解释**: “分枝”指的是将样本集划分为多个子集，形成层次结构。

6. **分枝定界算法的第二阶段是做什么？**
   - A. 构建树结构  
   - B. 划分样本子集  
   - C. 搜索最近邻样本  
   - D. 计算样本间的距离  
   - **正确答案**: C  
   - **解释**: 第二阶段是使用树状结构搜索未知样本的最近邻，从而提高搜索效率。

7. **在快速近邻法中，如何减少计算量？**
   - A. 通过并行计算  
   - B. 使用少量样本进行训练  
   - C. 通过层次划分跳过不相关的节点  
   - D. 仅考虑相似的样本  
   - **正确答案**: C  
   - **解释**: 通过层次划分样本集，算法可以在搜索时跳过不可能包含最近邻的节点，从而减少计算量。

8. **节点的属性“$r_p$”表示什么？**
   - A. 节点的样本数量  
   - B. 从节点中心到最远样本的距离  
   - C. 从节点中心到最近样本的距离  
   - D. 节点的样本平均值  
   - **正确答案**: B  
   - **解释**: $r_p$ 表示从节点中心到该节点样本集中最远样本的距离。

9. **快速近邻法中使用树状结构的主要目的是什么？**
   - A. 通过排序提高准确率  
   - B. 减少存储空间  
   - C. 加速最近邻的搜索过程  
   - D. 增加分类器的复杂度  
   - **正确答案**: C  
   - **解释**: 使用树状结构的主要目的是加速搜索过程，减少新样本与每个样本逐一比较的计算量。

10. **近邻法的计算复杂度在未使用快速算法时是什么量级？**
    - A. 线性时间复杂度  
    - B. 二次时间复杂度  
    - C. 对数时间复杂度  
    - D. 常数时间复杂度  
    - **正确答案**: B  
    - **解释**: 近邻法需要计算每个新样本与所有已知样本的距离，计算复杂度为 $O(N^2)$ 的二次复杂度。

11. **分枝定界算法的第一阶段主要是为了做什么？**
    - A. 构建判别函数  
    - B. 划分样本集形成树状结构  
    - C. 选择最近邻的样本  
    - D. 排序样本集  
    - **正确答案**: B  
    - **解释**: 第一阶段是将样本集划分为多个子集，形成树状结构，以便在搜索时减少计算量。

12. **快速近邻法适用于哪种情况？**
    - A. 样本数据较少时  
    - B. 样本数据量较大时  
    - C. 仅用于回归问题  
    - D. 仅用于线性分类问题  
    - **正确答案**: B  
    - **解释**: 快速近邻法主要用于样本数据量较大的情况，以减少搜索最近邻样本的计算量。

13. **在快速近邻法中，如何判断是否需要深入某个子集？**
    - A. 通过计算子集的平均值  
    - B. 通过比较节点的中心与新样本的距离  
    - C. 通过随机选择节点  
    - D. 通过计算样本的标准差  
    - **正确答案**: B  
    - **解释**: 通过比较新样本与

节点中心的距离，可以判断是否需要进一步搜索该子集。

14. **快速近邻法的主要优势是什么？**
    - A. 可以处理高维数据  
    - B. 大幅减少了计算时间  
    - C. 提高了分类精度  
    - D. 适用于小规模数据集  
    - **正确答案**: B  
    - **解释**: 快速近邻法通过分层划分数据集，显著减少了计算时间。

15. **在快速近邻法中，第二阶段使用了哪种方法来加速搜索？**
    - A. 回溯法  
    - B. 贪心算法  
    - C. 层次搜索  
    - D. 聚类分析  
    - **正确答案**: C  
    - **解释**: 第二阶段使用层次搜索，逐层排除不相关的节点，从而加速搜索过程。

16. **快速近邻法中的“分枝”过程指的是什么？**
    - A. 对样本进行分类  
    - B. 对样本集进行层次划分  
    - C. 优化搜索算法  
    - D. 训练分类器  
    - **正确答案**: B  
    - **解释**: “分枝”过程指的是对样本集进行层次划分，以便形成树状结构进行快速搜索。

17. **分枝定界算法的第二阶段的任务是？**
    - A. 更新样本集  
    - B. 构建分类模型  
    - C. 搜索最优路径  
    - D. 搜索最近邻样本  
    - **正确答案**: D  
    - **解释**: 第二阶段的任务是使用分枝定界的树状结构搜索新样本的最近邻。

18. **在快速近邻法中，如何减少计算每个样本的距离？**
    - A. 通过缩小搜索范围  
    - B. 通过增加样本数量  
    - C. 通过删除部分样本  
    - D. 通过使用特征选择  
    - **正确答案**: A  
    - **解释**: 快速近邻法通过缩小搜索范围，只对可能包含最近邻的样本子集计算距离，从而减少计算量。

19. **分枝定界算法的搜索过程如何进行？**
    - A. 从根节点开始，逐层排除不相关节点  
    - B. 从叶节点开始，向上搜索  
    - C. 通过随机选择子集进行搜索  
    - D. 通过并行搜索进行优化  
    - **正确答案**: A  
    - **解释**: 搜索过程从根节点开始，逐层排除不相关节点，最终定位到可能包含最近邻的子集。

20. **快速近邻法的计算复杂度相比标准近邻法如何？**
    - A. 更高  
    - B. 更低  
    - C. 相同  
    - D. 随样本数量变化而变化  
    - **正确答案**: B  
    - **解释**: 快速近邻法通过层次划分显著降低了计算复杂度，相比标准近邻法更高效。